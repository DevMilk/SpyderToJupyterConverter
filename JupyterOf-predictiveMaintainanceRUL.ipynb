{"cells":[
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"# -*- coding: utf-8 -*-\n",
"\"\"\"\n",
"Created on Mon Jul 27 14:20:12 2020\n",
"\n",
"@author: Ugur\n",
"\"\"\"\n",
"import pandas as pd\n",
"import numpy as np\n",
"import matplotlib.pyplot as plt\n",
"import os\n",
" \n",
"figDir = \"Figures/\" \n"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"from sklearn import preprocessing\n",
"\n",
"def split(): print(\"\n____________________________________________________________________________________\n\")\n",
"\n",
"#Tüm featureler için korelasyon matrisi\n",
"def plotCorrelationMatrix(df, graphWidth):  \n",
"    df = df[[col for col in df if df[col].nunique() > 1]] # keep columns where there are more than 1 unique values\n",
"    if df.shape[1] < 2:\n",
"        print(f'No correlation plots shown: The number of non-NaN or constant columns ({df.shape[1]}) is less than 2')\n",
"        return\n",
"    corr = df.corr()\n",
"    plt.figure(num=None, figsize=(graphWidth, graphWidth), dpi=80, facecolor='w', edgecolor='k')\n",
"    corrMat = plt.matshow(corr, fignum = 1)\n",
"    plt.xticks(range(len(corr.columns)), corr.columns, rotation=90)\n",
"    plt.yticks(range(len(corr.columns)), corr.columns)\n",
"    plt.gca().xaxis.tick_bottom()\n",
"    plt.colorbar(corrMat)\n",
"    plt.title(f'Correlation Matrix for xd', fontsize=40)\n",
"    plt.show()\n",
"#Boxplot, gruplama ve korelasyonların hepsini analiz eden all-in-one fonksiyon\n",
"    plt.savefig(figDir+'CorrelationMatrix.png')\n",
"def intro(df,graph=True,splitPlots=True,EraseNullColumns=False,printCorrelations=True,corrThreshold=0.5):\n",
"    \n",
"    dataframe=df.copy()\n",
"    \n",
"    if(EraseNullColumns==True):  dataframe.dropna(axis=1,inplace=True)\n",
"\n",
"    split()\n",
"    print(df)\n",
"    split()\n",
"    print(dataframe.head(5))\n",
"    split()\n",
"    \n",
"    print(dataframe.info())\n",
"    split()\n",
"    \n",
"    print(dataframe.describe())\n",
"    split()\n",
"    \n",
"#-------------------------------BOXPLOTFEATURES-----------------------------      \n",
"    \n",
"    \n",
"    if(graph):\n",
"\n",
"        if(splitPlots==True):\n",
"            print(\"                         ___BOXPLOTFETURES\")\n",
"\n",
"            for column in dataframe.columns:\n",
"                if(dataframe[column].dtype==np.int or dataframe[column].dtype==np.float):\n",
"                    plt.figure()\n",
"                    dataframe.boxplot([column])\n",
"                    plt.savefig(figDir+'{}.png'.format(column))\n",
"                    \n",
"        else:\n",
"            dataframe.boxplot()\n",
"            \n",
"    #If unique values of columns is under 10, print unique values with considered column\n",
"\n",
"\n",
"#-------------------------------GROUPBY-----------------------------        \n",
"\n",
"    print(\"                         _____GROUPBY____\")\n",
"\n",
"    for column in dataframe.columns:    \n",
"        unique_values=dataframe[column].unique()\n",
"        if(unique_values.size<=10):\n",
"            print(column,\": \",unique_values)\n",
"            print(\"\nGrouped By: \",column,\"\n\n\",dataframe.groupby(column).mean())\n",
"            split()\n",
"            print(\"\n\")\n",
"            \n",
"        \n",
"#-------------------------------CORRELATIONS-----------------------------        \n",
"    if(printCorrelations==True):\n",
"        print(\"                         ____CORRELATIONS____\")\n",
"        corrByValues= dataframe.corr().copy()\n",
"        flag = False\n",
"        corr_matrix=abs(corrByValues>=corrThreshold)\n",
"        columns= corr_matrix.columns\n",
"        for i in range(columns.size):\n",
"            for j in range(i,columns.size):\n",
"                iIndex=columns[i]\n",
"                jIndex=columns[j] \n",
"                if (i!=j and corr_matrix[iIndex][jIndex]==True and (len(df[iIndex].unique())!=1 and len(df[jIndex].unique())!=1 )):\n",
"                    sign = \"Positive\"\n",
"                    if(corrByValues[iIndex][jIndex]<0): sign=\"Negative\"\n",
"                    split()\n",
"                    flag = True\n",
"                    print(iIndex.upper(), \" has a \" ,sign,\" correlation with \",jIndex.upper(),\": {} \n\".format(corrByValues[iIndex][jIndex]))\n",
"        \n",
"        plt.show()\n",
"        plotCorrelationMatrix(df,30)       \n",
"        \n",
"        split()\n",
"        if(not flag):\n",
"            print(\"No Correlation Found\") \n",
"    return dataframe\n",
"\n",
"#KDE dağılımı ile featureları plotlar\n",
"def plotCols(df,time):\n",
"    \n",
"    for col in df.columns:\n",
"        if(df[col].dtype==np.int or df[col].dtype==np.float):\n",
"            if(len(df[col].unique())>1):\n",
"                fig = df.plot(x=time,y=col,kind=\"kde\", title = \"{}-{} KDE\".format(time,col))    \n",
"                fig.get_figure().savefig(figDir+\"{}-kde.png\".format(time+\"-\"+col))\n",
"                plt.show()\n",
"            plt.plot(df[time],df[col]) \n",
"            plt.title(\"{}-{}\".format(time,col))\n",
"            plt.show()\n",
"            plt.savefig(figDir+'{}.png'.format(time+\"-\"+col))\n",
"        \n",
"#Verilen feature'ları scatter ile Y'ye göre karşılaştırır.        \n",
"def XCorrWithY(df, X, Y):\n",
"    for col in  X:\n",
"        print(col,\"-\",Y)\n",
"        plt.scatter(df[col],df[Y]) \n",
"        plt.title(\"{}-{}\".format(col,Y))\n",
"        plt.show()    \n",
"#Dataframeyi normalize eder. (Preprocessing)        \n",
"def normalizedf(df,offset=0):\n",
"    min_max_scaler = preprocessing.MinMaxScaler() \n",
"    new = df.copy()\n",
"    cols = df.columns[offset:]\n",
"    new[cols] = (min_max_scaler.fit_transform(new[cols]) )  \n",
"    return new    \n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Tüm Makinelerin verilerini import et"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"    \n",
"colnames = [\"unit_num\",\"time_in_cycles\" ]\n",
"for i in range(3):\n",
"    colnames.append(\"operational_setting{}\".format(i+1))\n",
"for i in range(21):\n",
"    colnames.append(\"s{}\".format(i+1))\n",
"    \n",
"def getData(prefix,num,names=colnames):\n",
"        return pd.read_csv('{}_FD00{}.txt'.format(prefix,num), delim_whitespace=True, header = None, names= names )  \n",
"        \n",
"traindfs = []\n",
"testdfs = [] \n",
"testYs = []\n",
"\n",
"for i in range(4):\n",
"    traindfs.append(getData(\"train\",i+1) ) \n",
"      \n",
"for i in range(4): \n",
"    testdfs.append(getData(\"test\",i+1) )       \n",
"     \n",
"for i in range(4):\n",
"    testYs.append(getData(\"RUL\",i+1,names=[\"Y\"]))       \n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Makinelerden birini seç"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"index = 3\n",
"traindf = traindfs[index]\n",
"testdf  = testdfs[index]\n",
"testY   = testYs[index]  \n",
"#           R^2   MAE     RMSE \n",
"# Index 0  0.74  17.27   21.27 : LSTM\n",
"# Index 0  0.79  15.09   18.8 : LSTM, CROP 50\n",
"# Index 0  0.72  17.27   27.09 : STACK MLP+LASSO -> MLP LOOK_BACK: 31\n",
"# Index 0  0.73  16.31   21.65 : LSTM, PADDING 50\n",
"# Index 1  0.61  25.66   33.64 : LGMRegressor           LOOK_BACK: 21\n",
"# Index 2  0.51  20.49   29.01 : STACK MLP+LASSO -> MLP LOOK_BACK: 35\n",
"# Index 2  0.62  19.57   25.66 : STACK MLP+LASSO -> MLP LOOK_BACK: 50 PADDING\n",
"# Index 3 -> look back = 19\n",
"# Denenecekler: Sabit look_back ile bu look_back'in altında kalan veriler gözardı edilecek\n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Veri ANALİZİ"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"intro(traindf)\n",
"plotCols(traindf,\"time_in_cycles\")\n",
"XCorrWithY(traindf,[\"s9\"],\"s14\")\n",
"\"\"\"\n",
"Time in cycles: Sensor2,3,4,11,15,17 ile pozitif korelasyonu bulunuyor \n",
"Sensor2'nin: Sensor 3,4,8,11,13,15,17\n",
"Sensor3'ün:  Sensor 4,8,9,11,13,15,17\n",
"Sensor4'ün: 8,11,13,15,17 \n",
"Sensor7'nin: 12,20,21\n",
"Sensor8'in: 11,13,15,17\n",
"Sensor9'un: 14(%96)  \n",
"Sensor11: 13,15,17\n",
"Sensor12: 20,21\n",
"Sensor13: 15,17\n",
"Sensor15: 17\n",
"Sensor20: 21\n",
"\n",
"\n",
"-Veride Null değer yok\n",
"-15 ve 17. sensörlerin çoğu ile korelasyonu vr \n",
"-operationalssetting3,s1,s5,10,s16,s18 ve s19'un train datasında sadece 1 değeri var; s6-> 2 değer \n",
"\"\"\"\n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Preprocessing for 001 "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"\n",
"#Sabit kalan feature'ları temizle\n",
"def removeSame(df,threshold=1):\n",
"    new = df.copy()\n",
"    willremove = [] \n",
"    for col in new.columns:\n",
"        if(len(new[col].unique())<=threshold):\n",
"            del new[col] \n",
"            willremove.append(col)\n",
"    return new,willremove\n",
"\n",
"#Korelasyonu fazla olan feature'leri temizle\n",
"def get_train_columns(df,CorrThreshold):\n",
"    corrByValues= df.corr().copy()\n",
"    corrMat = abs(corrByValues)>=CorrThreshold\n",
"    print(corrMat)\n",
"    columnList = df.columns.to_list()\n",
"    length = len(columnList)\n",
"    features = columnList\n",
"    for i in range(length):\n",
"        for j in range(i+1,length):\n",
"            if(corrMat.iloc[i,j]==True and df.columns[j] in features):\n",
"                features.remove(df.columns[j])\n",
"    return features  \n",
"\n",
"#Time-Series bir şekilde train ve test verilerini ayarla\n",
"def create_dataset(dataset, look_back=1):\n",
"\tdataX, dataY = [], []\n",
"\tfor i in range(len(dataset)-look_back):\n",
"\t\ta = dataset.iloc[i:(i+look_back), :-1]\n",
"\t\tdataX.append(a.to_numpy())\n",
"\t\tdataY.append(dataset.iloc[i + look_back, -1]) \n",
"\treturn np.array(dataX), np.array(dataY)  \n",
"trainX = traindf\n",
"testX = testdf\n",
"#Her devirde Sabit kalan değerleri çıkar \n",
"trainX,deletedCols = removeSame(traindf.iloc[:,:],threshold=1)\n",
"testX = testdf.drop(columns=deletedCols)   \n",
"\n",
"#Korelasyonu başka feature'lardan belirli seviteen fazla olan fetureları çıkar \n",
"input_features = get_train_columns(trainX,0.75) \n",
"trainX, testX = trainX[input_features], testX[input_features]\n",
"#Her unit için çalıştığı en fazla devir\n",
"trainYs = traindf.groupby([\"unit_num\"]).time_in_cycles.max() \n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Method 2"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"CROP = True\n",
"PADDING = True \n",
"#Prepare train for all units without intercepting with each other     \n",
"look_back = 50 #min(trainYs.min(),testX.groupby([\"unit_num\"]).time_in_cycles.max().min())\n",
"assert look_back<=min(trainYs.min(),testX.groupby([\"unit_num\"]).time_in_cycles.max().min()) or CROP\n",
"unitnums = trainX.unit_num.unique() \n",
"def Padding(tX):\n",
"    unitnums = tX.unit_num.unique()\n",
"    dfPerUnit = []\n",
"    for i in unitnums:\n",
"        maxCycle = int(tX.loc[tX.unit_num==i].time_in_cycles.max())\n",
"        tmp = tX.loc[tX.unit_num==i]\n",
"        if(maxCycle<look_back):\n",
"            for j in range(look_back-maxCycle):\n",
"                tmp = tmp.append(tmp.iloc[-1] , ignore_index= True)     \n",
"                tmp.iloc[-1].time_in_cycles = maxCycle + j + 1 \n",
"        dfPerUnit.append(tmp)\n",
"    return pd.concat(dfPerUnit)    \n",
"    \n",
"X = trainX.copy()\n",
"Y = trainYs.copy()\n",
"tX = testX.copy()\n",
"tY = testY.copy()\n",
"if(PADDING):\n",
"    tX = Padding(tX)\n",
"    X = Padding(X) \n",
"\n",
"X[\"time\"] = X[\"time_in_cycles\"]\n",
"tX[\"time\"] = tX[\"time_in_cycles\"]\n",
"\n",
"X, tX = normalizedf(X,offset=2).fillna(0), normalizedf(tX,offset=2).fillna(0) \n",
"\n",
"\n",
"X[\"Y\"]=X.time_in_cycles\n",
"for i in range(len(unitnums)): \n",
"    X[\"Y\"].loc[X.unit_num==unitnums[i]] -= Y.iloc[i] \n",
"    X[\"Y\"]= abs(X[\"Y\"]) \n",
"\n",
"       \n",
"    \n",
"\n",
" \n",
"#Padding: En arkadaki değere look_back-size kadar aynı değeri ekle, cycle değerlerini yeni eklenenlere göre düzenle  \n",
"unitnums = trainX.unit_num.unique()    \n",
" \n",
"x, y = create_dataset(X.loc[X.unit_num==unitnums[0]].iloc[:,2:],look_back)\n",
"arrX = x\n",
"arrY = y\n",
"for i in range(1,len(unitnums)): \n",
"    machineData = X.loc[X.unit_num==unitnums[i]]\n",
"    if(CROP and machineData.shape[0]>look_back):\n",
"        x, y = create_dataset(machineData.iloc[:,2:],look_back)\n",
"        arrX = np.vstack((arrX,x))\n",
"        arrY = np.append(arrY,y)\n",
"print(tX.shape,X.shape)\n",
"\n",
"#Prepare test for all units without intercepting with each other    \n",
"    \n",
"testarrX = [tX[tX['unit_num']==id].values[-look_back:] for id in unitnums if (CROP and tX[tX['unit_num']==id].shape[0]>=look_back)]\n",
"print(tX.shape,X.shape)\n",
"\n",
"if(not testarrX[-1].shape[0]):\n",
"    testarrX = testarrX[:-1]\n",
"testarrX = np.asarray(testarrX ).astype(np.float32)\n",
"\n",
"tX = testarrX[:,:,2:] \n",
"remainingIdsAfterCrop = testarrX[:,0,0].astype(int)\n",
"X = arrX\n",
"Y = arrY \n",
"#edit tY for remainingIds after Crop. (-1 for mapping ids to indexes)\n",
"tY = testY.iloc[remainingIdsAfterCrop-1].Y\n",
"featureCount = X.shape[2] \n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Check Shapes"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"print(\"Train shape: \",X.shape,\" \", Y.shape,\"\nTest shape:\",tX.shape,\" \",tY.shape)\n",
"np.random.seed(42)\n",
"assert(X.shape[1:]==tX.shape[1:]) \n"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"track = pd.DataFrame(data={\"Machine Type\":[],\"batch_size\":[],\"modelSum\":[],\"look_back\":[],\"optimizer\":[],\"lr\":lr,\"epochs\":[],\"history\":[],\"RMSE\":[],\"r2\":[]})\n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" LSTM "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"from keras.models import Sequential,load_model\n",
"from keras.layers import Dense, LSTM, Dropout, LeakyReLU  \n",
"\n",
"from keras.optimizers import Adam, SGD, Adamax,RMSprop  \n",
"import tensorflow as tf \n",
"from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
"\"\"\"batch_size = 1000\n",
"epochs = 200\n",
"lr = 0.001\n",
"optimizer = Adam \n",
"\n",
"model = Sequential()\n",
"model.add(LSTM(200, input_shape=(look_back,featureCount ),return_sequences=True ,activation=\"linear\"))  \n",
"model.add(Dropout(0.2))\n",
"model.add(LSTM(100, input_shape=(look_back,featureCount ),return_sequences=False ,activation=\"linear\"))  \n",
"model.add(Dropout(0.2)) \n",
"model.add(Dense(1 ),activation=\"relu\") \n",
"model.compile(loss='mse', optimizer=optimizer(lr=lr) ,metrics=[\"mae\"] )\n",
"history = model.fit(X, Y, validation_data= (tX,tY),epochs=epochs, batch_size=batch_size, verbose=2,shuffle=True,use_multiprocessing=True,\n",
"                    callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=50, verbose=0, mode='min'),\n",
"                                  ModelCheckpoint(\"model.h5\",monitor='val_loss', save_best_only=True, mode='min', verbose=0)]) \n",
"\n",
"\n",
"\"\"\" \n",
"model = Sequential()\n",
"model.add(LSTM(\n",
"         input_shape=(look_back, featureCount),\n",
"         units=100,\n",
"         return_sequences=True))\n",
"model.add(Dropout(0.2))\n",
"model.add(LSTM(\n",
"          units=50,\n",
"          return_sequences=False))\n",
"model.add(Dropout(0.2)) \n",
"model.add(Dense(units=1,activation=\"linear\")) \n",
"model.compile(loss='mean_squared_error', optimizer='rmsprop',metrics=['mae'  ])\n",
"\n",
"print(model.summary())\n",
"\n",
"# fit the network\n",
"history = model.fit(X, Y, epochs=100, batch_size=200 ,  validation_data=(tX,tY), verbose=2,use_multiprocessing=True,\n",
"          callbacks = [EarlyStopping(monitor='val_loss', min_delta=0, patience=30, verbose=0, mode='min'),\n",
"                       ModelCheckpoint(\"modelCROP.h5\",monitor='val_loss', save_best_only=True, mode='min', verbose=1)]\n",
"          )\n",
"\n",
"#Plot History\n",
"plt.plot(history.history[\"loss\"])\n",
"plt.plot(history.history[\"val_loss\"])\n",
"plt.title(\"LSTM Training\")\n",
"plt.show()\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" ML Preprocesssing and testing functionss "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"import math \n",
"from sklearn.metrics import mean_squared_error,mean_absolute_error,r2_score \n",
"def Reshape3D(X):\n",
"    return np.reshape(X,(X.shape[0], featureCount*look_back))  \n",
"def testModel(model,MtX,tY): \n",
"    testPredict = model.predict(MtX)\n",
"    testPredict = np.reshape(testPredict,testPredict.shape[0]) \n",
"    tY = tY.astype(\"float\")\n",
"    testPredict = testPredict.astype(\"float\")  \n",
"    testScore = (mean_absolute_error(tY, testPredict))\n",
"    root_mse = math.sqrt(mean_squared_error(tY,testPredict))\n",
"    r2score = r2_score(tY,testPredict)\n",
"    print(str(model)+'\nTest Score: %.2f MAE' % (testScore))\n",
"    print('Test Score: %.2f RMSE' % (root_mse))\n",
"    print('Test Score: %.2f r2' % (r2score)) \n",
"    plt.plot(testPredict)\n",
"    plt.plot(tY)\n",
"    plt.title(str(model))\n",
"    plt.show()\n",
"    return testScore,r2score\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" TEst for LSTM "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"from keras.models import load_model\n",
"model = load_model(\"modelCROP.h5\",compile=True)\n",
"testModel(model,tX,tY)\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"trackML = pd.DataFrame(data={\"MachineType\":[],\"Model\":[],\"look_back\":[],\"RMSE\":[],\"r2\":[]})\n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Machine Learning Training and Test       "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"from sklearn.linear_model import LinearRegression, Lasso,Ridge, BayesianRidge \n",
"import xgboost as xgb\n",
"from sklearn import svm\n",
"from sklearn import tree \n",
"from sklearn.ensemble import GradientBoostingRegressor\n",
"from sklearn.neural_network import MLPRegressor\n",
"from lightgbm import LGBMRegressor\n",
"from catboost import CatBoostRegressor\n",
"models = [LGBMRegressor()]#LinearRegression(), xgb.XGBRegressor(),Lasso(),MLPRegressor(max_iter=500),LGBMRegressor(),CatBoostRegressor( ),Ridge(),BayesianRidge(),tree.DecisionTreeRegressor(),svm.SVR(),GradientBoostingRegressor()] \n",
"MtX = Reshape3D(tX)  \n",
"MX = Reshape3D(X)\n",
"testScores = []\n",
"r2scores = []\n",
"for MLmodel in models:\n",
"    MLmodel = MLmodel.fit(MX,Y)\n",
"    testScore,r2score = testModel(MLmodel,MtX,tY)\n",
"    testScores.append(testScore)\n",
"    r2scores.append(r2score)\n",
"# make predictions \n",
"\n"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"for i in range(len(models)):\n",
"    \n",
"    trackML = trackML.append({\"MachineType\":index,\"Model\":models[i],\"look_back\":look_back,\"RMSE\":testScores[i],\"r2\":r2scores[i]},ignore_index=True)\n",
"    \n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" Stacking Ensembling Machine Learning"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"# make a prediction with a stacking ensemble\n",
"from sklearn.datasets import make_regression\n",
"from sklearn.linear_model import LinearRegression\n",
"from sklearn.neighbors import KNeighborsRegressor\n",
"from sklearn.tree import DecisionTreeRegressor\n",
"from sklearn.svm import SVR\n",
"from sklearn.ensemble import StackingRegressor\n",
"# define dataset\n",
"# lasso + mlp -> mlp = 0.66 r^2 18.77 MAE,  eğer 30 devire bakarssa 0.68, cv=2, iter = 200 \n",
"# lasso + mlp -> mlp = 0.7 r^2, 17.57 MAE, 22.76 RMSE, 31 look_back, 250 iterasyon, 0.75 feature threshold, removesame , cv=2-> Paper'daki en iyi sonuçtan daha başarılı\n",
"# lasso + mlp -> mlp = 0.71 r^2, 17.03 MAE, 22.50 RMSE, 31 look_back, 300 iterasyon, 0.75 feature threshold, removesame , cv=2-> Paper'daki en iyi sonuçtan daha başarılı\n",
"# lasso + mlp -> mlp = 0.72 r^2, 17.27 MAE, 22.09 RMSE, 31 look_back, 300 iterasyon, 0.75 feature threshold, removesame th=2, cv=2-> Paper'daki en iyi sonuçtan daha başarılı\n",
"\n",
"# Lasso + mlp -> svm = 0.68 r^2, 17.61 MAE, 30 devir cv=2\n",
"# 31 devir, Lasso + mlp -> mlp = 0.67 r^2, 18.42 MAE\n",
"# define the base models\n",
"def ScatterPredictions(models,X,Y):\n",
"    axis = np.arange(X.shape[0]) \n",
"    fig = plt.figure()\n",
"    ax1 = fig.add_subplot(111)\n",
"    ax1.scatter(axis, Y, s=10,  label=\"REAL Y\", c='#FF4500')   \n",
"    for model in models: \n",
"        ax1.scatter(axis, model.predict(X), s=10,  label=str(model)[:10]) \n",
"    \n",
"    plt.title(\"Comparison of model predictions\")\n",
"    plt.show()  \n",
"    \n",
"MtX = Reshape3D(tX)  \n",
"MX = Reshape3D(X)\n",
"print(MX.shape,Y.shape,MtX.shape,tY.shape)\n",
"max_iter = 300\n",
"\"\"\"\n",
"models = [Lasso().fit(MX,Y), MLPRegressor(max_iter=max_iter).fit(MX,Y)]\n",
"ScatterPredictions(models,MX,Y) \"\"\" \n",
"level0 = list()\n",
"level0.append(('lasso', Lasso())) \n",
"level0.append(('mlp', MLPRegressor(max_iter=max_iter)))    \n",
"# define meta learner model\n",
"level1 = MLPRegressor(max_iter=max_iter )\n",
"# define the stacking ensemble\n",
"model = StackingRegressor(estimators=level0, final_estimator=level1, cv=2)\n",
"# fit the model on all available data\n",
"model =  model.fit(MX, Y)\n",
"# make a prediction for one example\n",
"testModel(model,MtX,tY)\n",
"\n",
"\"\"\"\"\"\"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" ANALYSIS ABOUT REGRESSION"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"from sklearn.inspection import permutation_importance\n",
"result = permutation_importance(model, X, Y, n_repeats=10,\n",
"                                random_state=42, n_jobs=2)\n",
"sorted_idx = result.importances_mean.argsort()\n",
"\n",
"fig, ax = plt.subplots()\n",
"ax.boxplot(result.importances[sorted_idx].T,\n",
"           vert=False, labels=trainX.columns[sorted_idx])\n",
"ax.set_title(\"Permutation Importances (test set)\")\n",
"fig.tight_layout()\n",
"plt.show()\n",
"\n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" SAVE MODEL"]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"\n",
"from sklearn.externals import joblib \n",
"  \n",
"# Save the model as a pickle in a file \n",
"joblib.dump(model, ' .pkl') \n",
"\n"]
},
{
 "cell_type": "markdown",
 "metadata": {},
 "source": [
" LOAD MODEL "]
},
{ "cell_type": "code",
 "execution_count": null,
 "metadata": {},
 "outputs": [],
 "source": [
"model = joblib.load(\"model71.pkl\")\n"]
}
],
 "metadata": {
 "kernelspec": {
 "display_name": "Python 3",
 "language": "python",
 "name": "python3"
  },
 "language_info": {
 "codemirror_mode": {
 "name": "ipython",
 "version": 3
 },
 "file_extension": ".py",
 "mimetype": "text/x-python",
 "name": "python",
 "nbconvert_exporter": "python",
 "pygments_lexer": "ipython3",
 "version": "3.6.4"
 }
 },
 "nbformat": 4,
 "nbformat_minor": 2}